# AI Inference for Unreferenced Tables - Master Document
**Date:** 2025-11-03
**Status:** ‚ö†Ô∏è **CRITICAL ISSUE DISCOVERED** - AI Inference Failing (12% Accuracy)
**Current Version:** Iteration 3 (0% hallucinations, but only 8/207 successful matches)
**Target Accuracy:** **>90%** (per user requirement)
**Current Accuracy:** **~12%** (1 correct out of 8 successful matches)
**Working Directory:** `/home/chris/sandbox/sqlglot_improvement/`
**Main Branch:** `feature/prod-cleanup`

---

## üö® CRITICAL VALIDATION FINDINGS (2025-11-03)

### The "207 AI-inferred tables" claim is MISLEADING

**Reality Check:**
- **Total tables with AI primary_source:** 207
- **Tables with actual SP matches (non-empty outputs):** 8 (3.9%)
- **Tables with NO matches (empty outputs):** 199 (96.1%)
- **Accuracy of the 8 successful matches:** 1 correct, 7 questionable = **12.5% accuracy**
- **Overall AI inference success rate:** ~2% (1 out of 46 unreferenced tables)

### What's Actually Happening

**CORRECTED ANALYSIS AFTER VALIDATION:**

The AI inference system has ONE CRITICAL PROBLEM: **FALSE POSITIVES**

1. **Empty Arrays Are CORRECT (True Negatives):**
   - AI processed 46 unreferenced tables
   - 45 returned empty arrays - **THESE ARE CORRECT** ‚úÖ
   - Manual verification: Those tables have NO SP references in SQL code
   - Only 1 table got a match (2.2% success rate is expected for this dataset)

2. **Matches Are WRONG (False Positives):** ‚ö†Ô∏è
   - Of 8 tables with matches, **7 are FALSE POSITIVES** (87.5% error rate)
   - Example: `ADMIN.Logs` matched to 4 SPs, but NONE of those SPs reference `Logs` in their SQL
   - Manual verification confirmed: The matched SPs do NOT contain the table name in their definitions
   - **Only 1 match is correct:** `FactLaborCostForEarnedValue_Post_Before` ‚Üí `spLoadFactLaborCostForEarnedValue_Post`

3. **Root Cause:**
   - AI is matching based on **contextual similarity** rather than **actual SQL code**
   - Tables like `Logs`, `Parameters`, `PipelineSerialization` are being matched to control/admin SPs
   - The prompt says "check SQL code" but AI is still using semantic/naming patterns

### Detailed Breakdown of 8 "Successful" Matches

| Table | SPs Matched | Verdict | Reason |
|-------|-------------|---------|--------|
| `CONSUMPTION_ClinOpsFinance.FactLaborCostForEarnedValue_Post_Before` | 1 | ‚úÖ CORRECT | Name matches SP pattern |
| `ADMIN.SecurityProcessingStatementSnippets` | 1 | ‚ö†Ô∏è QUESTIONABLE | Name mismatch with `sp_SetUpControlsHistoricalSnapshotRun` |
| `ADMIN.PipelineSerialization` | 3 | ‚ö†Ô∏è QUESTIONABLE | All 3 SPs have name mismatches |
| `ADMIN.PipelineTask` | 4 | ‚ö†Ô∏è QUESTIONABLE | All 4 SPs have name mismatches |
| `ADMIN.Logs` | 4 | ‚ö†Ô∏è QUESTIONABLE | All 4 SPs have name mismatches |
| `ADMIN.ETL_CONTROL` | 5 | ‚ö†Ô∏è QUESTIONABLE | All 5 SPs have name mismatches |
| `ADMIN.Parameters` | 4 | ‚ö†Ô∏è QUESTIONABLE | All 4 SPs have name mismatches |
| `ADMIN.ETL_CONTROL_SOURCE` | 3 | ‚ö†Ô∏è QUESTIONABLE | All 3 SPs have name mismatches |

**Accuracy:** 1/8 = **12.5%** ‚ùå

---

## ‚úÖ ITERATION 3 SUCCESS: Hallucination Problem SOLVED (But New Problem Found)

### Results Summary (2025-11-03)

**‚úÖ Hallucinations:** 0% (down from 69%) - **ELIMINATED**
**üìä AI-Inferred Tables:** 207 tables received AI lineage metadata
**‚ö†Ô∏è Success Rate (Unreferenced):** 2.2% (1/46 tables) - AI being very conservative
**üéØ Next Goal:** Increase success rate while maintaining 0% hallucinations

### What Changed in Iteration 3

**1. Enum Constraint in JSON Schema** (`ai_disambiguator.py:573-633`)
```python
"items": {
    "type": "string",
    "enum": sp_enum_list  # ‚Üê Forces AI to select only from provided SP catalog
}
```

**2. Explicit CRITICAL CONSTRAINTS** (`inference_prompt.txt:115-138`)
- Rule 1: ONLY return SP names from provided list
- Rule 2: Extract table names from ACTUAL SQL CODE
- Rule 3: If no match, return empty arrays
- Rule 4: Match table names EXACTLY
- Rule 5: Pluralization matters

**3. Negative Examples** (`inference_prompt.txt:72-113`)
- Example 4: Pattern exists but SQL doesn't match
- Example 5: SP name misleading - check actual SQL

**Result:** Enum constraint + explicit instructions = **0 hallucinations** ‚úÖ

### Critical Discovery from Analysis

**The 45 "failed" inferences are actually CORRECT REJECTIONS:**
- AI returned `confidence=0.0` with reasoning: "No SQL match found"
- This is desired behavior (true negatives), not failures
- AI is being conservative to avoid false positives

**The 207 AI-inferred tables show broader usage:**
- Confidence range: 0.75-0.95
- Mostly input procedures (reads FROM table)
- Few output procedures (writes TO table)
- Pattern: `Table: SCHEMA.TableName` ‚Üí `SP: SCHEMA.spLoadTableName`

---

## üö® CRITICAL: File Organization

**Working Directory:** `/home/chris/sandbox/sqlglot_improvement/` ONLY

**File Locations:**
- Docs ‚Üí `sqlglot_improvement/docs/`
- Scripts ‚Üí `sqlglot_improvement/scripts/`
- Test logs ‚Üí `sqlglot_improvement/*.log`
- DuckDB ‚Üí `lineage_v3/lineage_workspace.duckdb` (never move)

**See:** `FILE_ORGANIZATION.md` for rules

**Subagent for Analysis:** Use `/sub_DL_AnalyzeAI` (saves 75% tokens, see `docs/AI_ANALYSIS_SUBAGENT_USAGE.md`)

---

## Quick Start (Resume Work)

**Current State:** Iteration 3 complete. Hallucinations eliminated (0%). AI-inferred 207 tables with 75-95% confidence. Next: Investigate broader AI usage patterns and validate accuracy.

**NEXT ACTIONS (Priority Order):**
1. **[ANALYSIS]** Use `/sub_DL_AnalyzeAI` subagent to analyze Iteration 3 results ‚úÖ DONE
2. **[VALIDATION]** Spot-check the 207 AI-inferred tables for accuracy
3. **[ITERATE]** If accuracy <90%, add few-shot examples based on real failures
4. **[TEST]** Iteration 4: Target 15-25% success rate for unreferenced tables
5. **[PRODUCTION]** If accuracy >90%, document and deploy to production

**Key Files:**
- **This document** (`sqlglot_improvement/docs/AI_INFERENCE_MASTER_2025_11_03.md`)
- **[NEW]** `AI_ANALYSIS_SUBAGENT_USAGE.md` - How to use /sub_DL_AnalyzeAI
- `AI_FAILURE_ANALYSIS_2025_11_03.md` - Iteration 2 hallucination analysis
- `ITERATION_3_STRATEGY_2025_11_03.md` - Enum constraint implementation
- `ITERATION_3_IMPLEMENTATION_COMPLETE.md` - Implementation details
- `../lineage_v3/parsers/ai_disambiguator.py:573-640` - Enum constraint code
- `../lineage_v3/ai_analyzer/inference_prompt.txt` - System prompt with constraints
- `README.md` - Working directory overview

---

## Executive Summary

**Problem:** Parser couldn't infer dependencies for unreferenced tables, leaving coverage gaps.

**Iteration History:**
- **Iteration 0-1:** AI received only SP names without SQL code ‚Üí Poor accuracy
- **Iteration 2:** Added SQL code + few-shot examples ‚Üí 16.4% accuracy, 69% hallucinations
- **Iteration 3:** Enum constraint + explicit instructions + negative examples ‚Üí **0% hallucinations** ‚úÖ

**Current State:**
- **Hallucinations:** Eliminated (0%)
- **AI-Inferred Tables:** 207 tables with 75-95% confidence
- **Unreferenced Table Success Rate:** 2.2% (1/46) - AI being conservative
- **Overall Parser Coverage:** 94.0%

**Key Achievement:** Enum constraint in JSON Schema physically prevents AI from generating non-existent SP names.

**Next Steps:** Validate accuracy of 207 AI-inferred tables, then iterate on few-shot examples if needed to reach >90% target.

---

## Current Development Status

### ‚úÖ What's Working

1. **SQL Code Extraction** - Successfully extracts INSERT/SELECT statements, converts to lowercase
2. **JSON Schema Validation** - 100% guaranteed structure with required reasoning field (0 parsing errors)
3. **Few-Shot Learning** - 3 diverse examples teach AI the pattern effectively
4. **Iterative Methodology** - Systematic testing approach validated improvements

### ‚ùå What Didn't Work (Critical Issues SOLVED)

1. **üö® HALLUCINATION Issue** - AI returned non-existent SP names (e.g., "spLoadFactAgingSAP" when it doesn't exist)
   - **Root Cause:** Only provided SP names, AI guessed based on patterns
   - **Solution:** Provide actual SQL code + validation against catalog ‚úÖ

2. **üö® JSON Parsing Errors** - 70% of responses were malformed JSON (unterminated strings, missing keys)
   - **Root Cause:** Basic JSON mode + max_tokens=500 too low
   - **Solution:** JSON Schema with `strict: true` + max_tokens=2000 ‚Üí 0% errors ‚úÖ

3. **üö® Few-Shot Ineffective** - 13 examples made AI too conservative (7.2% accuracy)
   - **Root Cause:** Too many negative examples, no SQL code in examples
   - **Solution:** 3 diverse examples with actual SQL code ‚Üí 16.4% accuracy ‚úÖ

4. **Naming Patterns Only** - 7.2% accuracy without SQL code (too many false negatives)
5. **Zero Few-Shot** - 3.1% accuracy without examples (AI doesn't know what to look for)

### ‚è≥ What's In Progress

1. **Testing in Production** - Need to validate on full 176-table dataset
2. **User Feedback Loop** - UAT testing for false positives/negatives
3. **Baseline Comparison** - Use `/sub_DL_OptimizeParsing` to track metrics
4. **üé® GUI Filter Implementation** - See [SYSTEM_FILTERS_SPECIFICATION.md](SYSTEM_FILTERS_SPECIFICATION.md)
   - Need dropdown filter for: Unreferenced, Logging/Tracker, Placeholder objects
   - Reference: Frontend filter for new AI-inferred table categories
5. **üìã Placeholder Object Handling** - Tables intentionally without dependencies (Lookup tables, etc.)
   - Need to identify and exclude from AI inference
   - Update filter to show/hide these separately

---

## Iterative Test Results

| Iteration | Configuration | Tables Tested | Success | Rate | Improvement |
|-----------|--------------|---------------|---------|------|-------------|
| Baseline (old) | Names only + 13 examples | 69 | 5 | 7.2% | - |
| **Iteration 0** | SQL code + 0 examples | 64 | 2 | **3.1%** | Baseline |
| **Iteration 1** | SQL code + 1 example | 62 | 7 | **11.3%** | +264% |
| **Iteration 2** | SQL code + 3 examples + lowercase | 55 | 9 | **16.4%** | +429% ‚úÖ |

**Test Files:**
- `ai_test_iter0_sql_code.log` - Iteration 0
- `ai_test_iter1_one_example.log` - Iteration 1
- `ai_test_iter2_three_examples_lowercase.log` - Iteration 2 ‚≠ê

---

## Implementation Details

### Code Changes

**File:** `lineage_v3/parsers/ai_disambiguator.py`

**Changes Made:**
1. Added `_extract_sql_logic()` method to extract INSERT/SELECT/UPDATE/DELETE statements
2. Converts SQL to lowercase to reduce noise
3. Limits to 10 SP snippets (max 10 lines each) to stay within token limits
4. Implemented JSON Schema for 100% guaranteed structure

**Key Method:**
```python
def _extract_sql_logic(self, definition: str, max_lines: int = 15) -> str:
    """Extract INSERT/SELECT/UPDATE/DELETE, convert to lowercase."""
    # Filters out DECLARE, BEGIN TRY, error handling, logging
    # Returns only data manipulation statements in lowercase
```

### Prompt Configuration

**File:** `lineage_v3/ai_analyzer/inference_prompt.txt` (currently: iter2)

**Iteration 2 Configuration:**
- **System Prompt:** 3 diverse few-shot examples
- **User Prompt:** Target table + 10 SPs with lowercase SQL snippets
- **JSON Schema:** Enforces `input_procedures`, `output_procedures`, `confidence`, `reasoning`
- **Temperature:** 0.0 (fully deterministic)
- **Max Tokens:** 2000

**Examples Included:**
1. Positive match: INSERT INTO target table
2. Negative match: SQL inserts into different table
3. Read operation: SELECT FROM target table

---

## Critical Issue: Hallucination Prevention

**Problem:** AI was "hallucinating" - returning SP names that don't exist in the catalog.

**Example Hallucinations:**
- Returned: `"CONSUMPTION_FINANCE.spLoadFactAgingSAP"`
- Reality: This SP doesn't exist in our database!

**Root Causes:**
1. AI only saw SP names ‚Üí guessed based on naming patterns
2. No validation layer ‚Üí hallucinated SPs passed through unchecked
3. No SQL code ‚Üí AI couldn't verify table references

**Solutions Implemented:**
1. ‚úÖ **Provide SQL Code** - AI sees actual INSERT/SELECT statements, can't guess
2. ‚úÖ **Catalog Validation** - `_resolve_sp_names_to_ids()` checks SP exists (lineage_v3/parsers/ai_disambiguator.py:606-607)
3. ‚úÖ **JSON Schema with Reasoning** - Forces AI to explain its decision based on SQL code
4. ‚úÖ **Warning Logs** - "SP not found in catalog" alerts when AI returns invalid SP

**Current Status:** Hallucination prevented by catalog validation layer ‚úÖ

---

## Key Technical Findings

### 1. SQL Code is Essential

**Research:** 2024 best practices show few-shot prompts should include full code examples, not just names.

**Impact:** Without SQL code, AI can't see INSERT/SELECT statements ‚Üí relies only on naming patterns ‚Üí many false negatives.

**Evidence:** Iteration 0 (no SQL) = 3.1% vs Iteration 1 (with SQL) = 11.3%

### 2. Few-Shot Examples Critical

**Finding:** Zero-shot performs poorly (3.1%), but even 1 example dramatically improves results (11.3%).

**Optimal:** 2-3 diverse examples (positive + negative + edge case) = 16.4%

**Too Many:** 13 examples made AI too conservative, dropped from 47.7% ‚Üí 7.2%

### 3. Lowercase Normalization Benefits

**Research:** Reduces token dimensionality, improves pattern matching for SQL where case doesn't affect semantics.

**Impact:** Iteration 1 (mixed case) = 11.3% ‚Üí Iteration 2 (lowercase) = 16.4%

**Benefit:** `INSERT` vs `insert` vs `Insert` ‚Üí all become same pattern

### 4. JSON Schema Eliminates Parsing Errors

**Basic JSON Mode:** <40% reliability, frequent malformed responses

**JSON Schema with `strict: true`:** 100% compliance, 0 parsing errors

**Benefit:** AI **MUST** provide reasoning field, helps debug empty results

---

## Next Steps (Action List)

### üö® CRITICAL - Iteration 3 Implementation (IMMEDIATE)

1. **[30 min] Implement Enum Constraint** - `ai_disambiguator.py:592-600`
   - Build SP enum list from `all_sp_data`
   - Add enum to JSON Schema for `input_procedures` and `output_procedures`
   - **Expected:** Eliminates 100% of hallucinations

2. **[15 min] Add Explicit Instructions** - `inference_prompt.txt:72+`
   - Add "CRITICAL CONSTRAINTS" section
   - Emphasize: "ONLY return SP names from provided list"
   - **Expected:** Reinforces enum constraint, +10-15% accuracy

3. **[30 min] Add Negative Examples** - `inference_prompt.txt:Examples 4-5`
   - Example 4: Pattern exists but SQL doesn't match
   - Example 5: SP name misleading, check actual SQL
   - **Expected:** Teaches correct rejection, +5-10% accuracy

4. **[15 min] Test on 10 Tables**
   - Run: `venv/bin/python scripts/test_ai_inference_10.py`
   - Verify: Zero "SP not found in catalog" warnings
   - **Expected:** 50-60% accuracy, zero hallucinations

5. **[15 min] Test on Full 176-Table Dataset**
   - Run full parser with new configuration
   - Log: `ai_test_iter3_enum_constraint.log`
   - **Expected:** 60-75% accuracy

6. **[30 min] Iterate if <90%**
   - Analyze failures with `query_ai_results.py`
   - Add real failed cases to few-shot
   - Re-test until >90% achieved

### Short-Term (After >90% Achieved)

7. ‚è≥ **UAT Testing** - Let users validate/correct AI inferences
8. ‚è≥ **Collect Feedback** - Identify edge cases
9. ‚è≥ **Update Documentation** - Mark Iteration 3 as complete
10. ‚è≥ **Measure Coverage Improvement** - Track 84.2% ‚Üí 90-95% progress

---

## Related Documentation

### Core Implementation
- [../../docs/AI_DISAMBIGUATION_SPEC.md](../../docs/AI_DISAMBIGUATION_SPEC.md) - Original specification
- [../../lineage_v3/ai_analyzer/inference_prompt.txt](../../lineage_v3/ai_analyzer/inference_prompt.txt) - Current prompt (Iteration 2)
- [../../lineage_v3/parsers/ai_disambiguator.py:662-741](../../lineage_v3/parsers/ai_disambiguator.py) - Implementation

### Research & Findings (In This Directory)
- [FEWSHOT_RESEARCH_FINDINGS_2025_11_03.md](FEWSHOT_RESEARCH_FINDINGS_2025_11_03.md) - Academic research
- [JSON_SCHEMA_IMPLEMENTATION.md](JSON_SCHEMA_IMPLEMENTATION.md) - Structured outputs approach

### SQLGlot Background (In This Directory)
- [../README.md](../README.md) - Working directory overview
- [LOW_COVERAGE_FIX.md](LOW_COVERAGE_FIX.md) - Critical bug fix: 66.8% ‚Üí 84.2%
- [PATH_TO_95_PERCENT_COVERAGE.md](PATH_TO_95_PERCENT_COVERAGE.md) - Original roadmap (now archived)
- [PHASE1_FINAL_RESULTS.md](PHASE1_FINAL_RESULTS.md) - SQLGlot preprocessing experiments

### GUI Implementation (In This Directory)
- [SYSTEM_FILTERS_SPECIFICATION.md](SYSTEM_FILTERS_SPECIFICATION.md) - Frontend filter spec for AI-inferred categories

### Test Results (sqlglot_improvement/ Root)
- `../ai_test_iter0_sql_code.log` - Iteration 0: 3.1%
- `../ai_test_iter1_one_example.log` - Iteration 1: 11.3%
- `../ai_test_iter2_three_examples_lowercase.log` - Iteration 2: 16.4% ‚≠ê

### Helper Scripts (sqlglot_improvement/scripts/)
- `../scripts/query_ai_results.py` - Query AI inference results from workspace
- `../scripts/analyze_ai_results.py` - Analyze AI accuracy
- `../scripts/test_ai_inference_10.py` - Test AI on 10 sample tables

### Archived (Outdated)
- [../../docs/archive/2025-11-03_ai_iteration/](../../docs/archive/2025-11-03_ai_iteration/) - Old AI strategy documents
- [../../lineage_v3/ai_analyzer/archive/](../../lineage_v3/ai_analyzer/archive/) - Old prompt iterations

---

## Configuration Summary

### Current Production Settings

**Model:** `gpt-4.1-nano` (Azure OpenAI)
**API Version:** `2024-12-01-preview` (supports JSON Schema)
**Temperature:** `0.0` (deterministic)
**Max Tokens:** `2000` (per request)
**Token Rate Limit:** `1.5M TPM`

**Prompt Structure:**
- System: 3 few-shot examples (~800 tokens)
- User: Target table + 10 SP SQL snippets (~1200 tokens)
- Total: ~2000 input tokens per request

**Cost:** ~$0.38 per full run (176 tables √ó 2000 tokens)

---

## Success Criteria

### Minimum Success ‚úÖ (Achieved - Iteration 2)
- ‚úÖ Prove SQL code approach works (3.1% ‚Üí 16.4%)
- ‚úÖ Zero JSON parsing errors (100% with JSON Schema)
- ‚úÖ Iterative methodology validated
- ‚úÖ Identified critical hallucination problem (69% rate)

### Target Success üéØ (REQUIRED - Iteration 3)
- ‚è≥ **>90% accuracy on unreferenced tables** (per user requirement)
- ‚è≥ **Zero hallucinations** (enum constraint eliminates all invented names)
- ‚è≥ True positive rate: >80%
- ‚è≥ True negative rate: >95%
- ‚è≥ Coverage improves from 84.2% ‚Üí 90-95%

### Implementation Path (Research-Backed)
- **Phase 1:** Enum constraint ‚Üí 60-70% accuracy (eliminates hallucinations)
- **Phase 2:** Explicit instructions ‚Üí 70-80% accuracy (reinforces constraint)
- **Phase 3:** Negative examples + iteration ‚Üí >90% accuracy (target met)

---

## Lessons Learned

1. **Start Simple, Iterate** - Don't over-engineer; test minimal viable approach first
2. **Provide Context** - AI needs SQL code to make accurate inferences, not just names
3. **Measure Everything** - Systematic testing reveals what actually works
4. **Research Matters** - 2024 best practices (SQL code + few-shot) were correct
5. **One Change at a Time** - Iterative approach isolated what improved results

---

**Last Updated:** 2025-11-03
**Maintained By:** Claude Code
**Status:** ‚úÖ Ready for production testing with Iteration 2 configuration
