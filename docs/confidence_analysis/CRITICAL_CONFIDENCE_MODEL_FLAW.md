# CRITICAL FINDING: Confidence Model Design Flaw

**Date:** 2025-11-06
**Discovered By:** User review of Phase 2 validation testing
**Severity:** HIGH
**Impact:** Confidence scores may be inaccurate

---

## The Problem

The confidence scoring model measures **AGREEMENT between methods** instead of **ACCURACY of results**.

### Concrete Example

**Test Case:** `spLoadFactLaborCostForEarnedValue`
- **Regex Parser:** 100% accurate (found all 9/9 tables correctly)
- **SQLGlot Parser:** 0% accurate (failed completely on T-SQL complexity)
- **Method Agreement:** 0% (they don't agree)
- **Current Confidence Score:** **0.50 (LOW)** âŒ

**Problem:** Regex got 100% accuracy, but confidence is LOW because SQLGlot failed!

---

## Root Cause

### Current Formula (confidence_calculator.py lines 313-315)

```python
# Factor 2: Method Agreement (25%)
# Weighted average: targets count more (60%) than sources (40%)
agreement_score = (source_match * 0.4) + (target_match * 0.6)
```

**This calculates:**
- `source_match` = How many source tables do regex and SQLGlot both find?
- `target_match` = How many target tables do regex and SQLGlot both find?

**What it measures:** Agreement between methods
**What it should measure:** Accuracy of extracted dependencies

---

## Why This Happened

The model was designed to catch:
1. âœ… **Both methods wrong** (agree but both incorrect)
2. âœ… **Methods disagree** (one is likely wrong)

But it has a blind spot:
3. âŒ **One method excellent, other fails** â†’ LOW confidence (WRONG!)

---

## Real-World Impact

### Scenario Matrix

| Regex | SQLGlot | Catalog | Current Conf | Should Be | Status |
|-------|---------|---------|--------------|-----------|---------|
| 100% | 100% | 100% | 0.75 (Medium) | 0.85 (High) | Wrong |
| 100% | 0% | 100% | 0.50 (Low) | 0.85 (High) | **VERY WRONG** |
| 0% | 0% | 0% | 0.50 (Low) | 0.50 (Low) | Correct |
| 80% | 70% | 75% | 0.50-0.75 | 0.50-0.75 | Correct |

### Test Results

```
============================================================
SCENARIO: Regex perfect (100%), SQLGlot fails (0%)
============================================================

Confidence Score: 0.5
Label: Low

Breakdown:
  Parse Success:      0.3000 (weight: 30%)
  Method Agreement:   0.0000 (weight: 25%)  â† PROBLEM!
  Catalog Validation: 0.2000 (weight: 20%)
  Comment Hints:      0.0000 (weight: 10%)
  UAT Validation:     0.0000 (weight: 15%)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL:              0.5000
  BUCKETED:           0.5 (Low)
```

**Analysis:**
- Regex found all tables correctly (verified by catalog validation = 100%)
- SQLGlot failed (T-SQL complexity)
- Agreement = 0% (no match between methods)
- Confidence penalized to 0.50 (LOW)
- **This is wrong!** Should be 0.85 (HIGH)

---

## Evidence from Validation Testing

During Phase 2 validation, we tested comment hints vs regex vs SQLGlot:

| Parser | Tables Detected | Accuracy | Confidence Score |
|--------|----------------|----------|------------------|
| **Regex** | 9/9 (100%) | âœ… 100% | Should be HIGH |
| **SQLGlot** | 0/9 (0%) | âŒ 0% | - |
| **Agreement** | 0% | N/A | - |
| **Current Model** | - | - | 0.50 (LOW) âŒ |

**Conclusion:** Current model penalizes accurate regex results when SQLGlot fails.

---

## Why Regex is Reliable

Regex parser uses patterns like:
```python
FROM\s+\[?([schema])\]?\.\[?([table])\]?
JOIN\s+\[?([schema])\]?\.\[?([table])\]?
INSERT\s+INTO\s+\[?([schema])\]?\.\[?([table])\]?
```

**Strengths:**
- âœ… Works on all standard SQL patterns
- âœ… Handles CTEs, complex JOINs, UNION
- âœ… Fast and reliable
- âœ… Low false positive rate (catalog validation confirms)

**Weaknesses:**
- âŒ Can't parse dynamic SQL
- âŒ Can't handle temp tables (#temp)
- âŒ Can't resolve variables (@table)

**For standard SQL:** Regex is 90-100% accurate (as proven by test)

---

## Why SQLGlot Struggles

SQLGlot test results:
```
'CREATE PROC [...] AS BEGIN -- DESCRIPTION: ...'
contains unsupported syntax. Falling back to parsing as a 'Command'.
```

**Issues:**
- âŒ T-SQL dialect support incomplete
- âŒ Complex stored procedure syntax not supported
- âŒ Falls back to "Command" parsing (treats whole SP as one statement)
- âŒ Can't extract table dependencies from "Command" nodes

**Result:** SQLGlot often returns 0 tables for complex T-SQL

---

## Proposed Solution

Replace "Method Agreement" (25%) with "Parse Quality" (25%):

### New Logic

```python
def calculate_parse_quality(
    source_match: float,
    target_match: float,
    catalog_validation_rate: float,
    regex_count: int,
    sqlglot_count: int
) -> float:
    """
    Calculate parse quality score (0.0-1.0).

    Strategy:
    1. Trust catalog validation as proxy for accuracy
    2. Use SQLGlot agreement as confidence booster (not penalty)
    3. Be conservative only when both catalog AND agreement are low
    """

    # If catalog validation is high (90%+), trust the results
    # This indicates low false positive rate
    if catalog_validation_rate >= 0.90:
        return catalog_validation_rate  # 0.90-1.0

    # If SQLGlot agrees with regex, boost confidence
    # This provides additional validation
    if source_match >= 0.80 and target_match >= 0.80:
        agreement = (source_match * 0.4) + (target_match * 0.6)
        # Weighted blend: 60% catalog + 40% agreement
        return min(1.0, catalog_validation_rate * 0.6 + agreement * 0.4)

    # If disagreement and low catalog validation, be conservative
    # This indicates potential parsing issues
    return catalog_validation_rate * 0.7
```

### Example Outcomes

| Scenario | Regex | SQLGlot | Catalog | Old Score | New Score | Label |
|----------|-------|---------|---------|-----------|-----------|-------|
| **Test case** | 100% | 0% | 100% | 0.50 | **1.0** | High âœ… |
| Both agree | 100% | 100% | 100% | 0.75 | **1.0** | High âœ… |
| Both poor | 50% | 50% | 50% | 0.50 | 0.35 | Low âœ… |
| Disagreement | 80% | 60% | 70% | 0.65 | 0.58 | Med âœ… |

### Confidence Calculation

```python
# OLD
confidence = 0.30 (parse) + 0.0 (agreement) + 0.20 (catalog) = 0.50 (LOW)

# NEW
confidence = 0.30 (parse) + 0.25 (quality=1.0) + 0.20 (catalog) = 0.75 (MEDIUM)

# NEW + Hints
confidence = 0.30 + 0.25 + 0.20 + 0.10 (hints) = 0.85 (HIGH)
```

---

## Impact Analysis

### Current System (95.5% high confidence)

We report 95.5% of objects have confidence â‰¥0.85, but:
- â“ How many are **false confidence**? (high score but wrong)
- â“ How many **should be higher**? (accurate but scored low)

**Unknown without ground truth validation!**

### After Fix

Expected changes:
- âœ… **Regex-only results** (catalog valid): 0.50 â†’ 0.75-0.85 (fairer)
- âœ… **Agreement results** (both work): 0.75 â†’ 0.85 (reward validation)
- âœ… **Poor results** (low catalog): Remain low (correctly flagged)

**Net effect:** More accurate confidence scores reflecting actual quality

---

## Validation Plan

### Step 1: Test Proposed Fix

Run new calculation on test SP:
```python
parse_quality = calculate_parse_quality(
    source_match=0.0,        # SQLGlot failed
    target_match=0.0,
    catalog_validation_rate=1.0,  # All tables exist
    regex_count=9,
    sqlglot_count=0
)
# Expected: 1.0 (catalog validation high)

confidence = 0.30 + (parse_quality * 0.25) + 0.20 = 0.75
# With hints: 0.75 + 0.10 = 0.85 (HIGH) âœ…
```

### Step 2: Validate Against Production

Check impact on 763 objects:
- How many change from LOW to MEDIUM/HIGH?
- How many stay the same?
- Are there any regressions (HIGH to LOW)?

### Step 3: UAT Validation

Sample 20 SPs with low confidence (0.50):
- Check actual accuracy
- Confirm if they should be higher
- Validate new scoring is better

---

## Recommendations

### Immediate (Phase 3 - This Week)

1. **Implement fix** in `confidence_calculator.py`
2. **Test** with validation SP (should get 0.85 with hints)
3. **Run evaluation** on all 763 objects
4. **Compare** old vs new scores
5. **Validate** sample of changed scores

### Short-term (Next 2 Weeks)

6. **Update documentation** to clarify confidence meaning
7. **Add confidence breakdown** to exports (show factors)
8. **Create confidence guide** for interpreting scores

### Long-term (Next Month)

9. **Collect UAT feedback** on accuracy
10. **Tune weights** based on real-world validation
11. **Consider ML model** if we get enough ground truth data

---

## Risk Assessment

### Risks

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| **New model still wrong** | LOW | HIGH | Validate against sample of SPs |
| **Catalog not good proxy** | MEDIUM | MEDIUM | Check false positive/negative rates |
| **Breaks existing reports** | HIGH | LOW | Document changes, update frontend |
| **Users confused by changes** | MEDIUM | LOW | Clear communication, doc updates |

### Benefits

| Benefit | Value |
|---------|-------|
| **More accurate scores** | HIGH |
| **Fairer to regex results** | HIGH |
| **Better user trust** | MEDIUM |
| **Clearer confidence meaning** | MEDIUM |

**Overall:** Benefits outweigh risks. Fix should be implemented.

---

## Technical Debt Note

This issue reveals **deeper architectural question**:

> "What does confidence really mean?"

Current definitions:
- **v1.0:** Agreement between methods
- **v2.0:** Multi-factor model (still uses agreement)
- **v3.0 (proposed):** Likelihood of accuracy

**Need to decide:**
- Is catalog validation sufficient proxy for accuracy?
- Should we require UAT validation for HIGH confidence?
- How do we handle edge cases (dynamic SQL, temp tables)?

This should be addressed in Phase 3 design review.

---

## References

- **Confidence Calculator:** `lineage_v3/utils/confidence_calculator.py` (lines 240-391)
- **Test Evidence:** `temp/test_comment_hints_validation.py`
- **Phase 2 Report:** `temp/COMMENT_HINTS_VALIDATION_REPORT.md`
- **User Question:** "but when regex is quite good then we should also rethink the scoring for it to currently it is 0.5, correct?"

---

## Status

- **Discovered:** 2025-11-06 (during Phase 2 validation review)
- **Priority:** HIGH (affects all confidence scores)
- **Assigned to:** Phase 3 - Multi-factor Confidence Scoring
- **Target fix:** This week

---

**Prepared by:** Claude Code Agent
**Date:** 2025-11-06
**Status:** ðŸš¨ **CRITICAL ISSUE - REQUIRES FIX IN PHASE 3**
