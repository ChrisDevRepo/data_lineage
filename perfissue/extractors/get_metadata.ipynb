{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synapse Metadata Extractor\n",
    "## Extract DMV metadata to Parquet files for Data Lineage Parser\n",
    "\n",
    "**Version:** 1.1.0 (Updated DMV queries)\n",
    "**Date:** 2025-11-12\n",
    "\n",
    "**Required Libraries:**\n",
    "- pyodbc\n",
    "- pandas\n",
    "- pyarrow\n",
    "\n",
    "**Output Files:**\n",
    "1. `objects.parquet` - Database objects (tables, views, SPs, functions)\n",
    "2. `dependencies.parquet` - Object dependencies\n",
    "3. `definitions.parquet` - DDL definitions\n",
    "4. `query_logs.parquet` - Query execution logs (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - UPDATE THESE VALUES\n",
    "SERVER = \"yourserver.sql.azuresynapse.net\"\n",
    "DATABASE = \"yourdatabase\"\n",
    "USERNAME = \"youruser\"\n",
    "PASSWORD = \"yourpassword\"\n",
    "OUTPUT_DIR = \"parquet_snapshots\"\n",
    "\n",
    "# Create output directory\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Server: {SERVER}\")\n",
    "print(f\"  Database: {DATABASE}\")\n",
    "print(f\"  Output Directory: {output_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to Synapse\n",
    "conn_str = (\n",
    "    f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n",
    "    f\"SERVER={SERVER};\"\n",
    "    f\"DATABASE={DATABASE};\"\n",
    "    f\"UID={USERNAME};\"\n",
    "    f\"PWD={PASSWORD};\"\n",
    "    f\"Encrypt=yes;\"\n",
    "    f\"TrustServerCertificate=no;\"\n",
    "    f\"Connection Timeout=30;\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"Connecting to {SERVER}...\")\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    print(f\"‚úÖ Connected successfully to database: {DATABASE}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1: Extract Objects (Tables, Views, SPs, Functions)\n",
    "\n",
    "**Changes from v1.0:**\n",
    "- Consolidated function types: TF/IF/FN ‚Üí \"Function\"\n",
    "- Only 4 object types: Table, View, Stored Procedure, Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_objects = \"\"\"\n",
    "SELECT\n",
    "    o.object_id,\n",
    "    s.name AS schema_name,\n",
    "    o.name AS object_name,\n",
    "    o.type AS type_code,\n",
    "    CASE o.type\n",
    "        WHEN 'U' THEN 'Table'\n",
    "        WHEN 'V' THEN 'View'\n",
    "        WHEN 'P' THEN 'Stored Procedure'\n",
    "        WHEN 'TF' THEN 'Function'\n",
    "        WHEN 'IF' THEN 'Function'\n",
    "        WHEN 'FN' THEN 'Function'\n",
    "        ELSE o.type_desc\n",
    "    END AS object_type,\n",
    "    o.create_date,\n",
    "    o.modify_date,\n",
    "    o.type_desc AS full_type_description\n",
    "FROM sys.objects o\n",
    "JOIN sys.schemas s ON o.schema_id = s.schema_id\n",
    "WHERE o.type IN ('U', 'V', 'P', 'TF', 'IF', 'FN')\n",
    "    AND o.is_ms_shipped = 0\n",
    "ORDER BY s.name, o.name\n",
    "\"\"\"\n",
    "\n",
    "print(\"Extracting objects (tables, views, SPs, functions)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "df_objects = pd.read_sql(query_objects, conn)\n",
    "\n",
    "elapsed = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"‚úÖ Extracted {len(df_objects):,} objects in {elapsed:.2f}s\")\n",
    "print(f\"\\nObject Type Distribution:\")\n",
    "print(df_objects['object_type'].value_counts())\n",
    "\n",
    "# Save to Parquet\n",
    "objects_file = output_path / 'objects.parquet'\n",
    "df_objects.to_parquet(objects_file, engine='pyarrow', compression='snappy', index=False)\n",
    "file_size = objects_file.stat().st_size / 1024\n",
    "print(f\"\\nüíæ Saved: {objects_file} ({file_size:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2: Extract Dependencies\n",
    "\n",
    "From `sys.sql_expression_dependencies` (DMV metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dependencies = \"\"\"\n",
    "SELECT\n",
    "    d.referencing_id AS referencing_object_id,\n",
    "    d.referenced_id AS referenced_object_id,\n",
    "    d.referenced_schema_name,\n",
    "    d.referenced_entity_name,\n",
    "    d.referenced_database_name,\n",
    "    d.is_ambiguous,\n",
    "    d.is_schema_bound_reference,\n",
    "    d.is_caller_dependent,\n",
    "    d.referencing_class_desc,\n",
    "    d.referenced_class_desc,\n",
    "    o1.type_desc AS referencing_type,\n",
    "    o2.type_desc AS referenced_type\n",
    "FROM sys.sql_expression_dependencies d\n",
    "JOIN sys.objects o1 ON d.referencing_id = o1.object_id\n",
    "LEFT JOIN sys.objects o2 ON d.referenced_id = o2.object_id\n",
    "WHERE d.referencing_id IS NOT NULL\n",
    "    AND o1.is_ms_shipped = 0\n",
    "ORDER BY d.referencing_id, d.referenced_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"Extracting dependencies...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "df_dependencies = pd.read_sql(query_dependencies, conn)\n",
    "\n",
    "elapsed = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"‚úÖ Extracted {len(df_dependencies):,} dependencies in {elapsed:.2f}s\")\n",
    "\n",
    "# Save to Parquet\n",
    "deps_file = output_path / 'dependencies.parquet'\n",
    "df_dependencies.to_parquet(deps_file, engine='pyarrow', compression='snappy', index=False)\n",
    "file_size = deps_file.stat().st_size / 1024\n",
    "print(f\"üíæ Saved: {deps_file} ({file_size:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3: Extract Definitions (DDL)\n",
    "\n",
    "From `sys.sql_modules` (stored procedure/view/function definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_definitions = \"\"\"\n",
    "SELECT\n",
    "    m.object_id,\n",
    "    o.name AS object_name,\n",
    "    s.name AS schema_name,\n",
    "    o.type_desc AS object_type,\n",
    "    m.definition,\n",
    "    m.uses_ansi_nulls,\n",
    "    m.uses_quoted_identifier,\n",
    "    m.is_schema_bound,\n",
    "    o.create_date,\n",
    "    o.modify_date\n",
    "FROM sys.sql_modules m\n",
    "JOIN sys.objects o ON m.object_id = o.object_id\n",
    "JOIN sys.schemas s ON o.schema_id = s.schema_id\n",
    "WHERE o.is_ms_shipped = 0\n",
    "ORDER BY s.name, o.name\n",
    "\"\"\"\n",
    "\n",
    "print(\"Extracting definitions (DDL)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "df_definitions = pd.read_sql(query_definitions, conn)\n",
    "\n",
    "elapsed = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"‚úÖ Extracted {len(df_definitions):,} definitions in {elapsed:.2f}s\")\n",
    "\n",
    "# Save to Parquet\n",
    "defs_file = output_path / 'definitions.parquet'\n",
    "df_definitions.to_parquet(defs_file, engine='pyarrow', compression='snappy', index=False)\n",
    "file_size = defs_file.stat().st_size / 1024\n",
    "print(f\"üíæ Saved: {defs_file} ({file_size:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4: Extract Query Logs (Optional)\n",
    "\n",
    "**Changes from v1.0:**\n",
    "- **Removed:** Label filter (not used in your environment)\n",
    "- **Removed:** DDL operations (CREATE/ALTER/DROP) - not helpful for lineage\n",
    "- **Removed:** Explicit SELECT/WITH exclusion (redundant with whitelist)\n",
    "\n",
    "**Includes ONLY:**\n",
    "- Stored procedure executions (EXEC/EXECUTE)\n",
    "- DML operations (INSERT/UPDATE/DELETE/MERGE/TRUNCATE)\n",
    "\n",
    "**Automatically Excludes:**\n",
    "- Ad-hoc SELECT queries (not in whitelist)\n",
    "- Ad-hoc WITH/CTE queries (not in whitelist)\n",
    "- DDL operations (not in whitelist)\n",
    "\n",
    "**Note:** This query requires elevated DMV permissions. Skip if access is restricted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_logs = \"\"\"\n",
    "SELECT TOP 10000\n",
    "    r.request_id,\n",
    "    r.session_id,\n",
    "    r.submit_time,\n",
    "    r.start_time,\n",
    "    r.end_time,\n",
    "    r.status,\n",
    "    r.command,\n",
    "    r.total_elapsed_time,\n",
    "    r.[label],\n",
    "    SUBSTRING(r.command, 1, 4000) AS command_text\n",
    "FROM sys.dm_pdw_exec_requests r\n",
    "WHERE r.command IS NOT NULL\n",
    "    AND r.command NOT LIKE '%sys.dm_pdw_exec_requests%'\n",
    "    AND r.status IN ('Completed', 'Failed')\n",
    "    AND r.submit_time >= DATEADD(day, -7, GETDATE())\n",
    "    AND (\n",
    "        -- Stored procedure executions (most important for lineage)\n",
    "        r.command LIKE 'EXEC %'\n",
    "        OR r.command LIKE 'EXECUTE %'\n",
    "\n",
    "        -- DML operations (data transformation)\n",
    "        OR r.command LIKE 'INSERT %'\n",
    "        OR r.command LIKE 'UPDATE %'\n",
    "        OR r.command LIKE 'DELETE %'\n",
    "        OR r.command LIKE 'MERGE %'\n",
    "        OR r.command LIKE 'TRUNCATE %'\n",
    "    )\n",
    "ORDER BY r.submit_time DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Extracting query logs (last 7 days, max 10,000)...\")\n",
    "print(\"Note: Requires elevated DMV permissions\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    df_logs = pd.read_sql(query_logs, conn)\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"‚úÖ Extracted {len(df_logs):,} query logs in {elapsed:.2f}s\")\n",
    "    \n",
    "    # Show command type distribution\n",
    "    print(f\"\\nQuery Type Distribution:\")\n",
    "    df_logs['command_type'] = df_logs['command'].str.split().str[0]\n",
    "    print(df_logs['command_type'].value_counts())\n",
    "    \n",
    "    # Save to Parquet\n",
    "    logs_file = output_path / 'query_logs.parquet'\n",
    "    df_logs.to_parquet(logs_file, engine='pyarrow', compression='snappy', index=False)\n",
    "    file_size = logs_file.stat().st_size / 1024\n",
    "    print(f\"\\nüíæ Saved: {logs_file} ({file_size:.2f} KB)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Query logs extraction failed (this is OK if DMV access restricted): {e}\")\n",
    "    print(\"   Continuing without query logs...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "conn.close()\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Extraction Complete!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# List generated files\n",
    "print(f\"\\nGenerated Files in {output_path.absolute()}:\")\n",
    "for file in sorted(output_path.glob('*.parquet')):\n",
    "    file_size = file.stat().st_size / 1024\n",
    "    print(f\"  ‚úì {file.name} ({file_size:.2f} KB)\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Upload these Parquet files via the UI\")\n",
    "print(f\"  2. Or use the API: curl -X POST http://localhost:8000/api/upload-parquet -F files=@{output_path}/objects.parquet ...\")\n",
    "print(f\"  3. Or run parser directly: python lineage_v3/main.py run --parquet {output_path}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
